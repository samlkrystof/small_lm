{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 16, 4, 32])\n",
      "torch.Size([8, 16, 4, 32])\n",
      "torch.Size([8, 16, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "expansion_rate = 4\n",
    "layer_idx = 1\n",
    "batch_size = 8\n",
    "sequence_length = 16\n",
    "d_model = 32\n",
    "\n",
    "input_tensor = torch.randn(batch_size, sequence_length, expansion_rate, d_model)\n",
    "\n",
    "\n",
    "a_m = torch.zeros((expansion_rate,1))\n",
    "a_m[layer_idx % expansion_rate] = 1.0\n",
    "\n",
    "a_r = torch.eye(expansion_rate)\n",
    "\n",
    "beta = torch.ones(expansion_rate)\n",
    "\n",
    "alpha = torch.cat([a_m, a_r], dim=1)\n",
    "\n",
    "output = alpha.transpose(-1, -2) @ input_tensor\n",
    "\n",
    "h0 = output[..., 0, :]\n",
    "hr = output[..., 1:, :]\n",
    "\n",
    "H = torch.einsum(\"n, bld -> blnd\", beta, h0)\n",
    "\n",
    "print(H.shape)\n",
    "print(hr.shape)\n",
    "\n",
    "\n",
    "out = H + hr\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 0 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew style time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeedup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_time\u001b[38;5;241m/\u001b[39mnew_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m \u001b[43mtest_efficiency\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 110\u001b[0m, in \u001b[0;36mtest_efficiency\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m    109\u001b[0m     mixed_h \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m@\u001b[39m x\n\u001b[0;32m--> 110\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblh,bln->blnh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m old_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Test new style with efficient einsum\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/torch/functional.py:385\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    387\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 0 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StaticHyperConnection(nn.Module):\n",
    "    def __init__(self, dim: int, rate: int, layer_id: int):\n",
    "        \"\"\"\n",
    "        Optimized static hyper-connections implementation.\n",
    "        \n",
    "        Args:\n",
    "            dim: Hidden dimension size\n",
    "            rate: Expansion rate (n in paper)\n",
    "            layer_id: Current layer index\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize static alpha (width connections)\n",
    "        init_alpha0 = torch.zeros((rate, 1))\n",
    "        init_alpha0[layer_id % rate, 0] = 1.0\n",
    "        alpha = torch.cat([init_alpha0, torch.eye(rate)], dim=1)\n",
    "        self.static_alpha = nn.Parameter(alpha)  # [rate, rate+1]\n",
    "        \n",
    "        # Initialize static beta (depth connections)\n",
    "        self.static_beta = nn.Parameter(torch.ones(rate))  # [rate]\n",
    "        \n",
    "    def forward(self, h: torch.Tensor, layer_fn: callable) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with optimized einsum operations.\n",
    "        \n",
    "        Args:\n",
    "            h: Input hidden states [batch, seq_len, rate, dim]\n",
    "            layer_fn: Transformer layer function\n",
    "        \"\"\"\n",
    "        # Width connections using efficient einsum\n",
    "        # 'ij,blid->bljd' maps:\n",
    "        #   i: input alpha rows\n",
    "        #   j: input alpha cols\n",
    "        #   b: batch\n",
    "        #   l: sequence length\n",
    "        #   d: hidden dimension\n",
    "        mixed_h = torch.einsum('ij,blid->bljd', self.static_alpha, h)\n",
    "        \n",
    "        # Layer computation\n",
    "        layer_out = layer_fn(mixed_h[..., 0, :])  # Use first vector as input\n",
    "        \n",
    "        # Depth connections using efficient einsum\n",
    "        # 'n,bld->blnd' maps:\n",
    "        #   n: expansion rate\n",
    "        #   b: batch\n",
    "        #   l: sequence length\n",
    "        #   d: hidden dimension\n",
    "        h = torch.einsum('n,bld->blnd', self.static_beta, layer_out) + mixed_h[..., 1:, :]\n",
    "        \n",
    "        return h\n",
    "\n",
    "class TransformerWithStaticHC(nn.Module):\n",
    "    def __init__(self, dim: int, num_layers: int, expansion_rate: int = 4):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.expansion_rate = expansion_rate\n",
    "        \n",
    "        # Initialize transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Initialize static hyper-connections\n",
    "        self.hyper_connections = nn.ModuleList([\n",
    "            StaticHyperConnection(dim, expansion_rate, i)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through transformer with optimized static hyper-connections.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, dim]\n",
    "        \"\"\"\n",
    "        # Initialize hyper hidden matrix H0\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        h = x.unsqueeze(2).expand(-1, -1, self.expansion_rate, -1)\n",
    "        \n",
    "        # Process through layers\n",
    "        for layer, hyper_conn in zip(self.layers, self.hyper_connections):\n",
    "            h = hyper_conn(h, layer)\n",
    "        \n",
    "        # Final output is sum of last hyper hidden vectors\n",
    "        return h.sum(dim=2)\n",
    "\n",
    "def test_efficiency():\n",
    "    \"\"\"Demonstrate the efficiency gain\"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 32\n",
    "    seq_len = 128\n",
    "    dim = 512\n",
    "    rate = 4\n",
    "    \n",
    "    # Create test input\n",
    "    x = torch.randn(batch_size, seq_len, rate, dim)\n",
    "    \n",
    "    # Test old style with broadcasting\n",
    "    alpha = torch.randn(rate, rate+1)[None, None, ...]\n",
    "    beta = torch.randn(rate)[None, None, ...]\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        mixed_h = alpha.transpose(-1, -2) @ x\n",
    "        h = torch.einsum('blh,bln->blnh', x, beta)\n",
    "    old_time = time.time() - start\n",
    "    \n",
    "    # Test new style with efficient einsum\n",
    "    alpha = torch.randn(rate, rate+1).cuda()\n",
    "    beta = torch.randn(rate).cuda()\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        mixed_h = torch.einsum('ij,blid->bljd', alpha, x)\n",
    "        h = torch.einsum('n,bld->blnd', beta, x)\n",
    "    new_time = time.time() - start\n",
    "    \n",
    "    print(f\"Old style time: {old_time:.3f}s\")\n",
    "    print(f\"New style time: {new_time:.3f}s\")\n",
    "    print(f\"Speedup: {old_time/new_time:.2f}x\")\n",
    "\n",
    "test_efficiency()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
